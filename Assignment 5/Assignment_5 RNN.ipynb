{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submitted by\n",
        "### Anurag Nagarkoti (239426), Wahab Haseeb Bhatti (239978), Suyash Gawandi (239716)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZVE7aMvC4iXK"
      },
      "outputs": [],
      "source": [
        "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
        "num_words = 20000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c07u7Z7s4opk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# look at some sequences. words have been replaced with arbitrary index mappings\n",
        "# 1 is a special \"beginning of sequence\" marker\n",
        "# infrequent words have been replaced by the index 2\n",
        "# actual words start with index 4, 3 is never used (???)\n",
        "train_sequences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AD6Elit34sTL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# labels are simply binary: sentiment can be positive or negative\n",
        "train_labels[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xHTMEyXW5KcQ"
      },
      "outputs": [],
      "source": [
        "# to restore words, load the word-to-index mapping\n",
        "word_to_index = tf.keras.datasets.imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vv25lUc_5ckG"
      },
      "outputs": [],
      "source": [
        "# invert to get index-to-word mapping\n",
        "index_to_word = dict((index, word) for (word, index) in word_to_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CYX6F3AX5hpV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNKNOWN is an amazing actor and now the same being director UNKNOWN father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the UNKNOWN of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we can convert a sequence to text by\n",
        "# - replacing each index by the respective word\n",
        "# - joining words together via spaces\n",
        "# note that we remove the beginning of sequence character and we have to subtract 3 from all indices\n",
        "# this is because, as mentioned above, the smallest indices are reserved for special characters\n",
        "# but for some reason this is not reflected in the mapping...\n",
        "\" \".join([index_to_word.get(index - 3, \"UNKNOWN\") for index in train_sequences[0][1:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m2lt9mE-9XO7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2494"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# solution is padding all sequences to the maximum length.\n",
        "# first find the maximum length\n",
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "677ZXcRu9nUe"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# overview over sequence lengths in the data\n",
        "# could also look at mean, median, standard deviation...\n",
        "plt.hist(sequence_lengths, bins=80)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lYr10G5M9rWX"
      },
      "outputs": [],
      "source": [
        "# luckily there is a convenient function for padding\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pXEICggj-OL-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-16 12:25:11.154052: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
            "2023-11-16 12:25:11.154076: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2023-11-16 12:25:11.154082: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "2023-11-16 12:25:11.154274: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-11-16 12:25:11.154534: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "# now we can create a dataset!\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IPTPy5Ff-Q_C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 2494)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# all sequences are... very long\n",
        "train_sequences_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ug0OSIGjf6ji"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len = 200\n",
        "batch_size = 100\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_sequences, maxlen=max_len)\n",
        "train_data = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_sequences_padded, train_labels)).shuffle(25000).batch(batch_size)\n",
        "train_sequences_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2ZW7YdDv_fRJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 'the'),\n",
              " (2, 'and'),\n",
              " (3, 'a'),\n",
              " (4, 'of'),\n",
              " (5, 'to'),\n",
              " (6, 'is'),\n",
              " (7, 'br'),\n",
              " (8, 'in'),\n",
              " (9, 'it'),\n",
              " (10, 'i'),\n",
              " (11, 'this'),\n",
              " (12, 'that'),\n",
              " (13, 'was'),\n",
              " (14, 'as'),\n",
              " (15, 'for'),\n",
              " (16, 'with'),\n",
              " (17, 'movie'),\n",
              " (18, 'but'),\n",
              " (19, 'film'),\n",
              " (20, 'on'),\n",
              " (21, 'not'),\n",
              " (22, 'you'),\n",
              " (23, 'are'),\n",
              " (24, 'his'),\n",
              " (25, 'have'),\n",
              " (26, 'he'),\n",
              " (27, 'be'),\n",
              " (28, 'one'),\n",
              " (29, 'all'),\n",
              " (30, 'at'),\n",
              " (31, 'by'),\n",
              " (32, 'an'),\n",
              " (33, 'they'),\n",
              " (34, 'who'),\n",
              " (35, 'so'),\n",
              " (36, 'from'),\n",
              " (37, 'like'),\n",
              " (38, 'her'),\n",
              " (39, 'or'),\n",
              " (40, 'just'),\n",
              " (41, 'about'),\n",
              " (42, \"it's\"),\n",
              " (43, 'out'),\n",
              " (44, 'has'),\n",
              " (45, 'if'),\n",
              " (46, 'some'),\n",
              " (47, 'there'),\n",
              " (48, 'what'),\n",
              " (49, 'good'),\n",
              " (50, 'more'),\n",
              " (51, 'when'),\n",
              " (52, 'very'),\n",
              " (53, 'up'),\n",
              " (54, 'no'),\n",
              " (55, 'time'),\n",
              " (56, 'she'),\n",
              " (57, 'even'),\n",
              " (58, 'my'),\n",
              " (59, 'would'),\n",
              " (60, 'which'),\n",
              " (61, 'only'),\n",
              " (62, 'story'),\n",
              " (63, 'really'),\n",
              " (64, 'see'),\n",
              " (65, 'their'),\n",
              " (66, 'had'),\n",
              " (67, 'can'),\n",
              " (68, 'were'),\n",
              " (69, 'me'),\n",
              " (70, 'well'),\n",
              " (71, 'than'),\n",
              " (72, 'we'),\n",
              " (73, 'much'),\n",
              " (74, 'been'),\n",
              " (75, 'bad'),\n",
              " (76, 'get'),\n",
              " (77, 'will'),\n",
              " (78, 'do'),\n",
              " (79, 'also'),\n",
              " (80, 'into'),\n",
              " (81, 'people'),\n",
              " (82, 'other'),\n",
              " (83, 'first'),\n",
              " (84, 'great'),\n",
              " (85, 'because'),\n",
              " (86, 'how'),\n",
              " (87, 'him'),\n",
              " (88, 'most'),\n",
              " (89, \"don't\"),\n",
              " (90, 'made'),\n",
              " (91, 'its'),\n",
              " (92, 'then'),\n",
              " (93, 'way'),\n",
              " (94, 'make'),\n",
              " (95, 'them'),\n",
              " (96, 'too'),\n",
              " (97, 'could'),\n",
              " (98, 'any'),\n",
              " (99, 'movies'),\n",
              " (100, 'after')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for fun, you can look at the word-index mappings.\n",
        "# in this case, the mapping was done according to word frequency.\n",
        "# you can pass reverse=True to sorted() to look at the least common words.\n",
        "sorted(index_to_word.items())[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences_padded]\n",
        "max_len = max(sequence_lengths)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u4fwUhqBACri"
      },
      "outputs": [],
      "source": [
        "# here is a high-level sketch for training RNNs\n",
        "\n",
        "batch_size=100\n",
        "hidden_size=200\n",
        "U = tf.Variable(np.random.normal(size=(num_words, hidden_size),scale=0.1),dtype=np.float32)\n",
        "W = tf.Variable(np.random.normal(size=(hidden_size, hidden_size),scale=0.1),dtype=np.float32)\n",
        "V = tf.Variable(np.random.normal(size=(hidden_size, 2),scale=0.1),dtype=np.float32)\n",
        "\n",
        "b = tf.Variable(np.zeros(hidden_size), dtype=np.float32)\n",
        "c = tf.Variable(np.zeros(2), dtype=np.float32)\n",
        "\n",
        "old_state = tf.Variable(tf.zeros([100, hidden_size]))\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(0.01)\n",
        "weights=[W, U, V, b, c]\n",
        "learning_rate=0.1\n",
        "# training loop -- same thing as before!!\n",
        "# our data is now slightly different (each batch of sequences has a time axis, which is kinda new)\n",
        "# but all the related changes are hidden away at lower levels\n",
        "@tf.function\n",
        "def train_loop():\n",
        "    batch=1\n",
        "    for sequence_batch, label_batch in train_data:\n",
        "        train_step(sequence_batch, label_batch, batch)\n",
        "        batch=batch+1\n",
        "\n",
        "\n",
        "# a single training step -- again, seems familiar?\n",
        "def train_step(sequences, labels, batch):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = rnn_loop(sequences)\n",
        "        loss =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=labels))\n",
        "\n",
        "    grads = tape.gradient(loss, weights)\n",
        "    optimizer.apply_gradients(zip(grads, weights))\n",
        "    preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
        "    print(\"Batch {} Training Loss: {} Accuracy: {}\".format(batch,loss, acc))\n",
        "\n",
        "# here's where things start to change\n",
        "# we loop over the input time axis, and at each time step compute the new\n",
        "# hidden state based on the previous one as well as the current input\n",
        "# the state computation is hidden away in the rnn_step function and could be\n",
        "# arbitrarily complex.\n",
        "# in the general RNN, an output is computed at each time step, and the whole\n",
        "# sequence is returned. but in this case, since we only have one label for the\n",
        "# entire sequence, we only use the final state to compute one output and return it.\n",
        "# before the loop, the state need to be initialized somehow.\n",
        "def rnn_loop(sequences):\n",
        "\n",
        "    old_state = tf.zeros([100, hidden_size])\n",
        "    \n",
        "\n",
        "    for step in tf.range(max_len):\n",
        "        x_t = sequences[:, step] # sequences is batch x \n",
        "        x_t = tf.one_hot(x_t, depth=num_words)\n",
        "        new_state = rnn_step(old_state, x_t)\n",
        "        old_state = new_state\n",
        "\n",
        "    o_t = output_loop(new_state)\n",
        "\n",
        "    return o_t\n",
        "\n",
        "# see formulas in the book ;)\n",
        "def rnn_step(state, x_t):\n",
        "    at= b + tf.matmul(state,W) + tf.matmul(x_t, U)\n",
        "    state= tf.nn.tanh(at)\n",
        "    return state\n",
        "\n",
        "def output_loop(state):\n",
        "    o_t= c + tf.matmul(state, V)\n",
        "    return o_t\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Tensor(\"args_0:0\", shape=(), dtype=int32) Training Loss: Tensor(\"Mean:0\", shape=(), dtype=float32) Accuracy: Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
            "Batch Tensor(\"args_0:0\", shape=(), dtype=int32) Training Loss: Tensor(\"Mean:0\", shape=(), dtype=float32) Accuracy: Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-16 12:31:38.172238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-11-16 12:31:38.249622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-11-16 12:31:38.305177: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        }
      ],
      "source": [
        "train_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_sequences, maxlen=max_len)\n",
        "test_data = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_sequences_padded, test_labels)).batch(batch_size)\n",
        "test_sequences_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1 Training Loss: 0.6425560116767883 Accuracy: 0.6499999761581421\n",
            "Batch 2 Training Loss: 0.7212902903556824 Accuracy: 0.5199999809265137\n",
            "Batch 3 Training Loss: 0.6428536176681519 Accuracy: 0.5899999737739563\n",
            "Batch 4 Training Loss: 0.7031936645507812 Accuracy: 0.550000011920929\n",
            "Batch 5 Training Loss: 0.6836218237876892 Accuracy: 0.5299999713897705\n",
            "Batch 6 Training Loss: 0.6318041682243347 Accuracy: 0.6599999666213989\n",
            "Batch 7 Training Loss: 0.6912916302680969 Accuracy: 0.5099999904632568\n",
            "Batch 8 Training Loss: 0.6921981573104858 Accuracy: 0.5799999833106995\n",
            "Batch 9 Training Loss: 0.6861246228218079 Accuracy: 0.5600000023841858\n",
            "Batch 10 Training Loss: 0.6899120807647705 Accuracy: 0.550000011920929\n",
            "Batch 11 Training Loss: 0.7417776584625244 Accuracy: 0.5099999904632568\n",
            "Batch 12 Training Loss: 0.7159121036529541 Accuracy: 0.5600000023841858\n",
            "Batch 13 Training Loss: 0.7341687083244324 Accuracy: 0.5\n",
            "Batch 14 Training Loss: 0.6257787942886353 Accuracy: 0.6299999952316284\n",
            "Batch 15 Training Loss: 0.6383034586906433 Accuracy: 0.5199999809265137\n",
            "Batch 16 Training Loss: 0.680497407913208 Accuracy: 0.550000011920929\n",
            "Batch 17 Training Loss: 0.6732137799263 Accuracy: 0.5299999713897705\n",
            "Batch 18 Training Loss: 0.6652987599372864 Accuracy: 0.5600000023841858\n",
            "Batch 19 Training Loss: 0.6538568139076233 Accuracy: 0.5799999833106995\n",
            "Batch 20 Training Loss: 0.6536065340042114 Accuracy: 0.6200000047683716\n",
            "Batch 21 Training Loss: 0.6933834552764893 Accuracy: 0.5299999713897705\n",
            "Batch 22 Training Loss: 0.7389522194862366 Accuracy: 0.4399999976158142\n",
            "Batch 23 Training Loss: 0.6520348787307739 Accuracy: 0.5999999642372131\n",
            "Batch 24 Training Loss: 0.6749308705329895 Accuracy: 0.6100000143051147\n",
            "Batch 25 Training Loss: 0.677091658115387 Accuracy: 0.5600000023841858\n",
            "Batch 26 Training Loss: 0.6419318914413452 Accuracy: 0.6599999666213989\n",
            "Batch 27 Training Loss: 0.72458416223526 Accuracy: 0.5299999713897705\n",
            "Batch 28 Training Loss: 0.6781539916992188 Accuracy: 0.6200000047683716\n",
            "Batch 29 Training Loss: 0.7331068515777588 Accuracy: 0.5\n",
            "Batch 30 Training Loss: 0.6916133165359497 Accuracy: 0.5299999713897705\n",
            "Batch 31 Training Loss: 0.7082186937332153 Accuracy: 0.5600000023841858\n",
            "Batch 32 Training Loss: 0.6915915012359619 Accuracy: 0.5399999618530273\n",
            "Batch 33 Training Loss: 0.7124117612838745 Accuracy: 0.5199999809265137\n",
            "Batch 34 Training Loss: 0.6634888052940369 Accuracy: 0.5999999642372131\n",
            "Batch 35 Training Loss: 0.7012496590614319 Accuracy: 0.550000011920929\n",
            "Batch 36 Training Loss: 0.6600731611251831 Accuracy: 0.5699999928474426\n",
            "Batch 37 Training Loss: 0.7162232995033264 Accuracy: 0.5799999833106995\n",
            "Batch 38 Training Loss: 0.5932078957557678 Accuracy: 0.6699999570846558\n",
            "Batch 39 Training Loss: 0.7099030017852783 Accuracy: 0.5099999904632568\n",
            "Batch 40 Training Loss: 0.6936472654342651 Accuracy: 0.550000011920929\n",
            "Batch 41 Training Loss: 0.6305394768714905 Accuracy: 0.6100000143051147\n",
            "Batch 42 Training Loss: 0.7298380732536316 Accuracy: 0.5399999618530273\n",
            "Batch 43 Training Loss: 0.6804031133651733 Accuracy: 0.5199999809265137\n",
            "Batch 44 Training Loss: 0.6328120827674866 Accuracy: 0.6699999570846558\n",
            "Batch 45 Training Loss: 0.658764660358429 Accuracy: 0.550000011920929\n",
            "Batch 46 Training Loss: 0.6198660731315613 Accuracy: 0.6899999976158142\n",
            "Batch 47 Training Loss: 0.7058483958244324 Accuracy: 0.47999998927116394\n",
            "Batch 48 Training Loss: 0.7405505180358887 Accuracy: 0.5299999713897705\n",
            "Batch 49 Training Loss: 0.6297823786735535 Accuracy: 0.6399999856948853\n",
            "Batch 50 Training Loss: 0.7003825306892395 Accuracy: 0.5999999642372131\n",
            "Batch 51 Training Loss: 0.6913621425628662 Accuracy: 0.5699999928474426\n",
            "Batch 52 Training Loss: 0.6270029544830322 Accuracy: 0.6299999952316284\n",
            "Batch 53 Training Loss: 0.5842152833938599 Accuracy: 0.5999999642372131\n",
            "Batch 54 Training Loss: 0.6784040927886963 Accuracy: 0.5899999737739563\n",
            "Batch 55 Training Loss: 0.6668156385421753 Accuracy: 0.5899999737739563\n",
            "Batch 56 Training Loss: 0.6622176170349121 Accuracy: 0.6100000143051147\n",
            "Batch 57 Training Loss: 0.6600931882858276 Accuracy: 0.6499999761581421\n",
            "Batch 58 Training Loss: 0.6527850031852722 Accuracy: 0.5799999833106995\n",
            "Batch 59 Training Loss: 0.6335657238960266 Accuracy: 0.6299999952316284\n",
            "Batch 60 Training Loss: 0.6326798796653748 Accuracy: 0.5999999642372131\n",
            "Batch 61 Training Loss: 0.6949442028999329 Accuracy: 0.5600000023841858\n",
            "Batch 62 Training Loss: 0.6997343897819519 Accuracy: 0.5799999833106995\n",
            "Batch 63 Training Loss: 0.684853196144104 Accuracy: 0.5600000023841858\n",
            "Batch 64 Training Loss: 0.6515912413597107 Accuracy: 0.5899999737739563\n",
            "Batch 65 Training Loss: 0.6332502365112305 Accuracy: 0.6399999856948853\n",
            "Batch 66 Training Loss: 0.6641504764556885 Accuracy: 0.6100000143051147\n",
            "Batch 67 Training Loss: 0.6500258445739746 Accuracy: 0.5799999833106995\n",
            "Batch 68 Training Loss: 0.659991443157196 Accuracy: 0.5600000023841858\n",
            "Batch 69 Training Loss: 0.596693217754364 Accuracy: 0.6599999666213989\n",
            "Batch 70 Training Loss: 0.6647953391075134 Accuracy: 0.5600000023841858\n",
            "Batch 71 Training Loss: 0.7165383696556091 Accuracy: 0.5199999809265137\n",
            "Batch 72 Training Loss: 0.6801134943962097 Accuracy: 0.5299999713897705\n",
            "Batch 73 Training Loss: 0.6314236521720886 Accuracy: 0.5899999737739563\n",
            "Batch 74 Training Loss: 0.6616732478141785 Accuracy: 0.5600000023841858\n",
            "Batch 75 Training Loss: 0.6610180735588074 Accuracy: 0.5799999833106995\n",
            "Batch 76 Training Loss: 0.5951586365699768 Accuracy: 0.6299999952316284\n",
            "Batch 77 Training Loss: 0.7049204707145691 Accuracy: 0.550000011920929\n",
            "Batch 78 Training Loss: 0.6749836802482605 Accuracy: 0.5399999618530273\n",
            "Batch 79 Training Loss: 0.6535196304321289 Accuracy: 0.6100000143051147\n",
            "Batch 80 Training Loss: 0.6826661825180054 Accuracy: 0.5199999809265137\n",
            "Batch 81 Training Loss: 0.6749191284179688 Accuracy: 0.5399999618530273\n",
            "Batch 82 Training Loss: 0.7396676540374756 Accuracy: 0.4599999785423279\n",
            "Batch 83 Training Loss: 0.6367108225822449 Accuracy: 0.5799999833106995\n",
            "Batch 84 Training Loss: 0.6521238684654236 Accuracy: 0.5600000023841858\n",
            "Batch 85 Training Loss: 0.7214924693107605 Accuracy: 0.550000011920929\n",
            "Batch 86 Training Loss: 0.7486283183097839 Accuracy: 0.5099999904632568\n",
            "Batch 87 Training Loss: 0.5923911333084106 Accuracy: 0.6299999952316284\n",
            "Batch 88 Training Loss: 0.6402602195739746 Accuracy: 0.5999999642372131\n",
            "Batch 89 Training Loss: 0.5684807896614075 Accuracy: 0.6899999976158142\n",
            "Batch 90 Training Loss: 0.6450706124305725 Accuracy: 0.5799999833106995\n",
            "Batch 91 Training Loss: 0.6378351449966431 Accuracy: 0.5999999642372131\n",
            "Batch 92 Training Loss: 0.683818519115448 Accuracy: 0.5699999928474426\n",
            "Batch 93 Training Loss: 0.6799031496047974 Accuracy: 0.5399999618530273\n",
            "Batch 94 Training Loss: 0.6730918884277344 Accuracy: 0.5899999737739563\n",
            "Batch 95 Training Loss: 0.6662880778312683 Accuracy: 0.5399999618530273\n",
            "Batch 96 Training Loss: 0.5878435373306274 Accuracy: 0.6299999952316284\n",
            "Batch 97 Training Loss: 0.6588723659515381 Accuracy: 0.5799999833106995\n",
            "Batch 98 Training Loss: 0.6059161424636841 Accuracy: 0.6699999570846558\n",
            "Batch 99 Training Loss: 0.622962236404419 Accuracy: 0.6200000047683716\n",
            "Batch 100 Training Loss: 0.5944086313247681 Accuracy: 0.6699999570846558\n",
            "Batch 101 Training Loss: 0.6413611173629761 Accuracy: 0.5699999928474426\n",
            "Batch 102 Training Loss: 0.6745768785476685 Accuracy: 0.5899999737739563\n",
            "Batch 103 Training Loss: 0.691466212272644 Accuracy: 0.5600000023841858\n",
            "Batch 104 Training Loss: 0.6907409429550171 Accuracy: 0.5299999713897705\n",
            "Batch 105 Training Loss: 0.7133036851882935 Accuracy: 0.5299999713897705\n",
            "Batch 106 Training Loss: 0.5991120934486389 Accuracy: 0.6899999976158142\n",
            "Batch 107 Training Loss: 0.7044984102249146 Accuracy: 0.5399999618530273\n",
            "Batch 108 Training Loss: 0.6633338928222656 Accuracy: 0.5600000023841858\n",
            "Batch 109 Training Loss: 0.6630039811134338 Accuracy: 0.5899999737739563\n",
            "Batch 110 Training Loss: 0.6968032121658325 Accuracy: 0.5399999618530273\n",
            "Batch 111 Training Loss: 0.6828066110610962 Accuracy: 0.5299999713897705\n",
            "Batch 112 Training Loss: 0.662685215473175 Accuracy: 0.550000011920929\n",
            "Batch 113 Training Loss: 0.6493939161300659 Accuracy: 0.5899999737739563\n",
            "Batch 114 Training Loss: 0.6100982427597046 Accuracy: 0.6399999856948853\n",
            "Batch 115 Training Loss: 0.5938471555709839 Accuracy: 0.6399999856948853\n",
            "Batch 116 Training Loss: 0.622563898563385 Accuracy: 0.6299999952316284\n",
            "Batch 117 Training Loss: 0.7053443193435669 Accuracy: 0.5199999809265137\n",
            "Batch 118 Training Loss: 0.6946308612823486 Accuracy: 0.4699999988079071\n",
            "Batch 119 Training Loss: 0.6613568067550659 Accuracy: 0.5899999737739563\n",
            "Batch 120 Training Loss: 0.6753534078598022 Accuracy: 0.6200000047683716\n",
            "Batch 121 Training Loss: 0.6783591508865356 Accuracy: 0.5299999713897705\n",
            "Batch 122 Training Loss: 0.6284316778182983 Accuracy: 0.6299999952316284\n",
            "Batch 123 Training Loss: 0.649038553237915 Accuracy: 0.5699999928474426\n",
            "Batch 124 Training Loss: 0.727672278881073 Accuracy: 0.5\n",
            "Batch 125 Training Loss: 0.6523556113243103 Accuracy: 0.6200000047683716\n",
            "Batch 126 Training Loss: 0.7241799235343933 Accuracy: 0.5199999809265137\n",
            "Batch 127 Training Loss: 0.6477224826812744 Accuracy: 0.6100000143051147\n",
            "Batch 128 Training Loss: 0.6264614462852478 Accuracy: 0.6200000047683716\n",
            "Batch 129 Training Loss: 0.6817483305931091 Accuracy: 0.5899999737739563\n",
            "Batch 130 Training Loss: 0.6661456227302551 Accuracy: 0.5799999833106995\n",
            "Batch 131 Training Loss: 0.6167828440666199 Accuracy: 0.6399999856948853\n",
            "Batch 132 Training Loss: 0.6250373721122742 Accuracy: 0.6399999856948853\n",
            "Batch 133 Training Loss: 0.6809771656990051 Accuracy: 0.550000011920929\n",
            "Batch 134 Training Loss: 0.6398965120315552 Accuracy: 0.6200000047683716\n",
            "Batch 135 Training Loss: 0.6299144625663757 Accuracy: 0.6299999952316284\n",
            "Batch 136 Training Loss: 0.6494054794311523 Accuracy: 0.5600000023841858\n",
            "Batch 137 Training Loss: 0.6357337236404419 Accuracy: 0.6200000047683716\n",
            "Batch 138 Training Loss: 0.6182068586349487 Accuracy: 0.6599999666213989\n",
            "Batch 139 Training Loss: 0.6750186085700989 Accuracy: 0.5799999833106995\n",
            "Batch 140 Training Loss: 0.7125133275985718 Accuracy: 0.5199999809265137\n",
            "Batch 141 Training Loss: 0.6277962327003479 Accuracy: 0.5600000023841858\n",
            "Batch 142 Training Loss: 0.6256130337715149 Accuracy: 0.6100000143051147\n",
            "Batch 143 Training Loss: 0.7005727887153625 Accuracy: 0.550000011920929\n",
            "Batch 144 Training Loss: 0.5965282917022705 Accuracy: 0.6399999856948853\n",
            "Batch 145 Training Loss: 0.7172735333442688 Accuracy: 0.5399999618530273\n",
            "Batch 146 Training Loss: 0.6182178258895874 Accuracy: 0.6299999952316284\n",
            "Batch 147 Training Loss: 0.5939971208572388 Accuracy: 0.6599999666213989\n",
            "Batch 148 Training Loss: 0.5972195267677307 Accuracy: 0.6399999856948853\n",
            "Batch 149 Training Loss: 0.6451869010925293 Accuracy: 0.6100000143051147\n",
            "Batch 150 Training Loss: 0.6484165191650391 Accuracy: 0.5699999928474426\n",
            "Batch 151 Training Loss: 0.6618746519088745 Accuracy: 0.5399999618530273\n",
            "Batch 152 Training Loss: 0.6639028787612915 Accuracy: 0.5799999833106995\n",
            "Batch 153 Training Loss: 0.6696624755859375 Accuracy: 0.5899999737739563\n",
            "Batch 154 Training Loss: 0.68445885181427 Accuracy: 0.5600000023841858\n",
            "Batch 155 Training Loss: 0.6134219169616699 Accuracy: 0.6299999952316284\n",
            "Batch 156 Training Loss: 0.7056288123130798 Accuracy: 0.5199999809265137\n",
            "Batch 157 Training Loss: 0.6367334723472595 Accuracy: 0.6299999952316284\n",
            "Batch 158 Training Loss: 0.7192740440368652 Accuracy: 0.5299999713897705\n",
            "Batch 159 Training Loss: 0.5800458788871765 Accuracy: 0.6499999761581421\n",
            "Batch 160 Training Loss: 0.6942866444587708 Accuracy: 0.5600000023841858\n",
            "Batch 161 Training Loss: 0.690244734287262 Accuracy: 0.5699999928474426\n",
            "Batch 162 Training Loss: 0.7012528777122498 Accuracy: 0.5299999713897705\n",
            "Batch 163 Training Loss: 0.6044125556945801 Accuracy: 0.6299999952316284\n",
            "Batch 164 Training Loss: 0.6962614059448242 Accuracy: 0.550000011920929\n",
            "Batch 165 Training Loss: 0.6464729309082031 Accuracy: 0.5899999737739563\n",
            "Batch 166 Training Loss: 0.5932764410972595 Accuracy: 0.6699999570846558\n",
            "Batch 167 Training Loss: 0.7466537952423096 Accuracy: 0.5399999618530273\n",
            "Batch 168 Training Loss: 0.7047844529151917 Accuracy: 0.5699999928474426\n",
            "Batch 169 Training Loss: 0.6827622652053833 Accuracy: 0.5399999618530273\n",
            "Batch 170 Training Loss: 0.59963059425354 Accuracy: 0.6599999666213989\n",
            "Batch 171 Training Loss: 0.6362468004226685 Accuracy: 0.6699999570846558\n",
            "Batch 172 Training Loss: 0.7185343503952026 Accuracy: 0.5399999618530273\n",
            "Batch 173 Training Loss: 0.6519564390182495 Accuracy: 0.5999999642372131\n",
            "Batch 174 Training Loss: 0.6500822305679321 Accuracy: 0.5899999737739563\n",
            "Batch 175 Training Loss: 0.6849932074546814 Accuracy: 0.550000011920929\n",
            "Batch 176 Training Loss: 0.6467676162719727 Accuracy: 0.5799999833106995\n",
            "Batch 177 Training Loss: 0.7172053456306458 Accuracy: 0.44999998807907104\n",
            "Batch 178 Training Loss: 0.651628851890564 Accuracy: 0.5699999928474426\n",
            "Batch 179 Training Loss: 0.6541473865509033 Accuracy: 0.5999999642372131\n",
            "Batch 180 Training Loss: 0.5900499820709229 Accuracy: 0.6800000071525574\n",
            "Batch 181 Training Loss: 0.6488234400749207 Accuracy: 0.5999999642372131\n",
            "Batch 182 Training Loss: 0.6272785663604736 Accuracy: 0.5699999928474426\n",
            "Batch 183 Training Loss: 0.6831368207931519 Accuracy: 0.550000011920929\n",
            "Batch 184 Training Loss: 0.6705687642097473 Accuracy: 0.550000011920929\n",
            "Batch 185 Training Loss: 0.6001131534576416 Accuracy: 0.6200000047683716\n",
            "Batch 186 Training Loss: 0.6733352541923523 Accuracy: 0.5799999833106995\n",
            "Batch 187 Training Loss: 0.6812447905540466 Accuracy: 0.5199999809265137\n",
            "Batch 188 Training Loss: 0.6598045825958252 Accuracy: 0.5699999928474426\n",
            "Batch 189 Training Loss: 0.669177234172821 Accuracy: 0.5999999642372131\n",
            "Batch 190 Training Loss: 0.6061225533485413 Accuracy: 0.6100000143051147\n",
            "Batch 191 Training Loss: 0.7372104525566101 Accuracy: 0.5099999904632568\n",
            "Batch 192 Training Loss: 0.6520800590515137 Accuracy: 0.5799999833106995\n",
            "Batch 193 Training Loss: 0.6514174342155457 Accuracy: 0.5899999737739563\n",
            "Batch 194 Training Loss: 0.5798280239105225 Accuracy: 0.699999988079071\n",
            "Batch 195 Training Loss: 0.6805259585380554 Accuracy: 0.5399999618530273\n",
            "Batch 196 Training Loss: 0.6751037240028381 Accuracy: 0.5999999642372131\n",
            "Batch 197 Training Loss: 0.7426234483718872 Accuracy: 0.44999998807907104\n",
            "Batch 198 Training Loss: 0.6928945779800415 Accuracy: 0.5899999737739563\n",
            "Batch 199 Training Loss: 0.6952478885650635 Accuracy: 0.5799999833106995\n",
            "Batch 200 Training Loss: 0.6295523643493652 Accuracy: 0.6200000047683716\n",
            "Batch 201 Training Loss: 0.6259090304374695 Accuracy: 0.6399999856948853\n",
            "Batch 202 Training Loss: 0.7282848954200745 Accuracy: 0.550000011920929\n",
            "Batch 203 Training Loss: 0.6237322688102722 Accuracy: 0.5899999737739563\n",
            "Batch 204 Training Loss: 0.5885488986968994 Accuracy: 0.6699999570846558\n",
            "Batch 205 Training Loss: 0.6753529906272888 Accuracy: 0.5699999928474426\n",
            "Batch 206 Training Loss: 0.6405329704284668 Accuracy: 0.5799999833106995\n",
            "Batch 207 Training Loss: 0.6866215467453003 Accuracy: 0.5799999833106995\n",
            "Batch 208 Training Loss: 0.6910408139228821 Accuracy: 0.5799999833106995\n",
            "Batch 209 Training Loss: 0.6531577110290527 Accuracy: 0.5600000023841858\n",
            "Batch 210 Training Loss: 0.7064111232757568 Accuracy: 0.5699999928474426\n",
            "Batch 211 Training Loss: 0.5704574584960938 Accuracy: 0.6699999570846558\n",
            "Batch 212 Training Loss: 0.6220228672027588 Accuracy: 0.6399999856948853\n",
            "Batch 213 Training Loss: 0.6791602373123169 Accuracy: 0.5399999618530273\n",
            "Batch 214 Training Loss: 0.667060375213623 Accuracy: 0.5799999833106995\n",
            "Batch 215 Training Loss: 0.6097121238708496 Accuracy: 0.6699999570846558\n",
            "Batch 216 Training Loss: 0.6081913113594055 Accuracy: 0.6599999666213989\n",
            "Batch 217 Training Loss: 0.6025301814079285 Accuracy: 0.6399999856948853\n",
            "Batch 218 Training Loss: 0.6680940389633179 Accuracy: 0.5799999833106995\n",
            "Batch 219 Training Loss: 0.5767831802368164 Accuracy: 0.6499999761581421\n",
            "Batch 220 Training Loss: 0.6094066500663757 Accuracy: 0.5999999642372131\n",
            "Batch 221 Training Loss: 0.6089075803756714 Accuracy: 0.6100000143051147\n",
            "Batch 222 Training Loss: 0.7202086448669434 Accuracy: 0.5299999713897705\n",
            "Batch 223 Training Loss: 0.5966659188270569 Accuracy: 0.6699999570846558\n",
            "Batch 224 Training Loss: 0.7175962924957275 Accuracy: 0.5299999713897705\n",
            "Batch 225 Training Loss: 0.6567849516868591 Accuracy: 0.5999999642372131\n",
            "Batch 226 Training Loss: 0.6495106220245361 Accuracy: 0.5299999713897705\n",
            "Batch 227 Training Loss: 0.653025209903717 Accuracy: 0.5799999833106995\n",
            "Batch 228 Training Loss: 0.6320819854736328 Accuracy: 0.5699999928474426\n",
            "Batch 229 Training Loss: 0.625747561454773 Accuracy: 0.5899999737739563\n",
            "Batch 230 Training Loss: 0.600980281829834 Accuracy: 0.5999999642372131\n",
            "Batch 231 Training Loss: 0.6581119894981384 Accuracy: 0.5600000023841858\n",
            "Batch 232 Training Loss: 0.6611645221710205 Accuracy: 0.6100000143051147\n",
            "Batch 233 Training Loss: 0.641501247882843 Accuracy: 0.5899999737739563\n",
            "Batch 234 Training Loss: 0.7088721990585327 Accuracy: 0.4899999797344208\n",
            "Batch 235 Training Loss: 0.7321630716323853 Accuracy: 0.5600000023841858\n",
            "Batch 236 Training Loss: 0.6846994161605835 Accuracy: 0.5399999618530273\n",
            "Batch 237 Training Loss: 0.6485999822616577 Accuracy: 0.5699999928474426\n",
            "Batch 238 Training Loss: 0.6185855865478516 Accuracy: 0.6399999856948853\n",
            "Batch 239 Training Loss: 0.6788183450698853 Accuracy: 0.550000011920929\n",
            "Batch 240 Training Loss: 0.6624546647071838 Accuracy: 0.6200000047683716\n",
            "Batch 241 Training Loss: 0.6580537557601929 Accuracy: 0.5799999833106995\n",
            "Batch 242 Training Loss: 0.6549961566925049 Accuracy: 0.6200000047683716\n",
            "Batch 243 Training Loss: 0.651856541633606 Accuracy: 0.5799999833106995\n",
            "Batch 244 Training Loss: 0.5701683163642883 Accuracy: 0.6800000071525574\n",
            "Batch 245 Training Loss: 0.6983477473258972 Accuracy: 0.5399999618530273\n",
            "Batch 246 Training Loss: 0.6350967884063721 Accuracy: 0.6399999856948853\n",
            "Batch 247 Training Loss: 0.6775692701339722 Accuracy: 0.5199999809265137\n",
            "Batch 248 Training Loss: 0.6232696175575256 Accuracy: 0.6100000143051147\n",
            "Batch 249 Training Loss: 0.6402001976966858 Accuracy: 0.5699999928474426\n",
            "Batch 250 Training Loss: 0.6674115657806396 Accuracy: 0.550000011920929\n"
          ]
        }
      ],
      "source": [
        "def test_loop(sequences, labels, batch):\n",
        "    logits=rnn_loop(sequences)\n",
        "    loss =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, labels=labels))    \n",
        "    preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
        "    print(\"Batch {} Testing Loss: {} Accuracy: {}\".format(batch, loss, acc))\n",
        "\n",
        "batch=1\n",
        "for sequences, labels in test_data:\n",
        "    test_loop(sequences, labels, batch)\n",
        "    batch+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Food for thought #1: Why is this wasteful? Can you think of a smarter padding scheme that is more efficient? Consider the fact that RNNs can work on arbitrary sequence lengths, and that training minibatches are pretty much independent of each other.\n",
        "-  Learning on padding does not make sense as padding does not have any meaning\n",
        "-  smarter padding schemes: pad accoring to each batch, batch bucketing ie put sequences of similar length into a bucket of length size\n",
        "\n",
        "#### Food for thought #2: Between truncating long sequences and removing them, which option do you think is better? Why?\n",
        "- Truncating seems like the better option as we still retain some information than to completely lose it.\n",
        "- Truncating might not be good as the meaningful part of a sequences might be truncated\n",
        "\n",
        "#### Food for thought #3: Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.\n",
        "- TF-IDF, Word Embeddings\n",
        "\n",
        "#### Food for thought #4: How can it be that we can choose how many outputs we have, i.e. how can both be correct? Are there differences between both choices as well as (dis)advantages relative to each other?\n",
        "\n",
        "#### Food for thought #5: All sequences start with the same special “beginning of sequence” token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?\n",
        "\n",
        "\n",
        "#### Food for thought #6: pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better? Recall that we use the final time step output from our model.\n",
        "- prepadding \n",
        "\n",
        "#### Food for thought #7: Can you think of a way to prevent the RNN from computing new states on padded time steps? One idea might be to “pass through” the previous state in case the current time step is padding. Note that, within a batch, some sequences might be padded for a given time step while others are not.\n",
        "\n",
        "\n",
        "#### Food for thought #8: What could be the advantage of using methods like the above? What are disadvantages? Can you think of other methods to incorporate the full output sequence instead of just the final step?\n",
        "\n",
        "#### Cross entropy measures log probabilities which might be high even for instances classified correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
