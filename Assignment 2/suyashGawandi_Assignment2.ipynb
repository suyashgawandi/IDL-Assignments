{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anurag Nagarkoti (239426), Wahab Haseeb Bhatti (239978), Suyash Gawandi (239716)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up log dir and file writer(s)\n",
    "import os\n",
    "from datetime import datetime\n",
    "logdir = os.path.join(\"logs\", \"fail\" + str(datetime.now()))\n",
    "train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302590847015381 Accuracy: 0.0703125\n",
      "Loss: 2.3047566413879395 Accuracy: 0.0859375\n",
      "Loss: 2.3044583797454834 Accuracy: 0.1015625\n",
      "Loss: 2.304006576538086 Accuracy: 0.125\n",
      "Loss: 2.3008244037628174 Accuracy: 0.1015625\n",
      "Loss: 2.298145055770874 Accuracy: 0.1484375\n",
      "Loss: 2.292484998703003 Accuracy: 0.140625\n",
      "Loss: 2.2876052856445312 Accuracy: 0.15625\n",
      "Loss: 2.2033913135528564 Accuracy: 0.1953125\n",
      "Loss: 1.8391205072402954 Accuracy: 0.234375\n",
      "Loss: 1.8448164463043213 Accuracy: 0.1953125\n",
      "Loss: 1.4252291917800903 Accuracy: 0.40625\n",
      "Loss: 1.2602057456970215 Accuracy: 0.46875\n",
      "Loss: 1.7310227155685425 Accuracy: 0.3046875\n",
      "Loss: 1.1376526355743408 Accuracy: 0.6171875\n",
      "Loss: 0.907447874546051 Accuracy: 0.734375\n",
      "Loss: 0.6712876558303833 Accuracy: 0.765625\n",
      "Loss: 0.6052484512329102 Accuracy: 0.765625\n",
      "Loss: 0.7400422096252441 Accuracy: 0.7734375\n",
      "Loss: 0.516584038734436 Accuracy: 0.8515625\n",
      "Loss: 0.23684126138687134 Accuracy: 0.9453125\n",
      "Final test accuracy: 0.9134999513626099\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "\n",
    "# this is a super deep model, cool!\n",
    "n_units = 100\n",
    "n_layers = 8\n",
    "\n",
    "#reduced the range of weights from 0.4 to 0.1\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]\n",
    "\n",
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]\n",
    "\n",
    "\n",
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "    logits = (tf.matmul(x, layers[-1][0]) + layers[-1][1])\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "    \n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar(\"accuracy\", acc, step=step)\n",
    "        tf.summary.scalar(\"loss\", xent, step=step)\n",
    "        tf.summary.histogram(\"logits\", (logits), step=step)\n",
    "        tf.summary.histogram(\"weights\", (w_input), step=step)\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Exploding Gradient\n",
    "for gradients in range(0, 16, 2):\n",
    "    print(tf.reduce_mean(grads[gradients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Weights\n",
    "for layer in range(0, 8):\n",
    "    print(tf.reduce_mean(layers[layer][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 1 Solution:\n",
    "- Logits did not have the correct activation function for cross entropy loss\n",
    "- Also has larger weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 785), started 0:01:09 ago. (Use '!kill 785' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a7325c420a9fcbf0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a7325c420a9fcbf0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", \"fail\" + str(datetime.now()))\n",
    "train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3205761909484863 Accuracy: 0.0859375\n",
      "Loss: 2.3008761405944824 Accuracy: 0.125\n",
      "Loss: 2.2974891662597656 Accuracy: 0.078125\n",
      "Loss: 2.307187557220459 Accuracy: 0.0546875\n",
      "Loss: 2.2981297969818115 Accuracy: 0.1484375\n",
      "Loss: 2.2920022010803223 Accuracy: 0.1171875\n",
      "Loss: 2.3081936836242676 Accuracy: 0.0859375\n",
      "Loss: 2.3023176193237305 Accuracy: 0.1015625\n",
      "Loss: 2.2901558876037598 Accuracy: 0.140625\n",
      "Loss: 2.303689956665039 Accuracy: 0.125\n",
      "Loss: 2.30786395072937 Accuracy: 0.125\n",
      "Loss: 2.3128044605255127 Accuracy: 0.09375\n",
      "Loss: 2.3132457733154297 Accuracy: 0.078125\n",
      "Loss: 2.3168492317199707 Accuracy: 0.09375\n",
      "Loss: 2.304708480834961 Accuracy: 0.109375\n",
      "Loss: 2.312185764312744 Accuracy: 0.0625\n",
      "Loss: 2.306908130645752 Accuracy: 0.09375\n",
      "Loss: 2.3023087978363037 Accuracy: 0.15625\n",
      "Loss: 2.30953311920166 Accuracy: 0.046875\n",
      "Loss: 2.297300338745117 Accuracy: 0.1015625\n",
      "Loss: 2.300079345703125 Accuracy: 0.0859375\n",
      "Final test accuracy: 0.09799999743700027\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "\n",
    "# this is a super deep model, cool!\n",
    "n_units = 100\n",
    "n_layers = 8\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]\n",
    "\n",
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]\n",
    "\n",
    "\n",
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.sigmoid(tf.matmul(x, w) + b)\n",
    "    logits = (tf.matmul(x, layers[-1][0]) + layers[-1][1])\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "    \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
    "\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar(\"accuracy\", acc, step=step)\n",
    "            tf.summary.scalar(\"loss\", xent, step=step)\n",
    "            tf.summary.histogram(\"logits\", (logits), step=step)\n",
    "            tf.summary.histogram(\"weights\", (w_input), step=step)\n",
    "            for grad, var in zip(grads, all_variables):\n",
    "                tf.summary.histogram(var.name + '_grad', grad, step=step)\n",
    "\n",
    "\n",
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.712398e-11, shape=(), dtype=float32)\n",
      "tf.Tensor(-8.9114036e-11, shape=(), dtype=float32)\n",
      "tf.Tensor(4.444703e-09, shape=(), dtype=float32)\n",
      "tf.Tensor(2.9256881e-08, shape=(), dtype=float32)\n",
      "tf.Tensor(-2.7893222e-07, shape=(), dtype=float32)\n",
      "tf.Tensor(-2.359258e-06, shape=(), dtype=float32)\n",
      "tf.Tensor(-7.1668983e-06, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3815185e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Vanishing Gradient\n",
    "for gradients in range(0, 16, 2):\n",
    "    print(tf.reduce_mean(grads[gradients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 96972), started 0:02:42 ago. (Use '!kill 96972' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-578522e1a589de04\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-578522e1a589de04\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs_fail2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 2: Vanishing Gradient Problem\n",
    "- Fixed by changing the activation function from sigmoid to ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.4199376106262207 Accuracy: 0.09375\n",
      "Loss: 2.246652126312256 Accuracy: 0.1328125\n",
      "Loss: 2.3341312408447266 Accuracy: 0.09375\n",
      "Loss: 1.8768280744552612 Accuracy: 0.2890625\n",
      "Loss: 1.7602627277374268 Accuracy: 0.375\n",
      "Loss: 1.4352123737335205 Accuracy: 0.5078125\n",
      "Loss: 1.2670220136642456 Accuracy: 0.5078125\n",
      "Loss: 0.9759520292282104 Accuracy: 0.671875\n",
      "Loss: 0.7710796594619751 Accuracy: 0.8125\n",
      "Loss: 0.7946479320526123 Accuracy: 0.7421875\n",
      "Loss: 0.5637328624725342 Accuracy: 0.84375\n",
      "Loss: 0.5424056649208069 Accuracy: 0.859375\n",
      "Loss: 0.6422869563102722 Accuracy: 0.796875\n",
      "Loss: 0.4663069546222687 Accuracy: 0.8828125\n",
      "Loss: 0.43985897302627563 Accuracy: 0.859375\n",
      "Loss: 0.4139168858528137 Accuracy: 0.890625\n",
      "Loss: 0.5024144649505615 Accuracy: 0.84375\n",
      "Loss: 0.2682867646217346 Accuracy: 0.9296875\n",
      "Loss: 0.32801398634910583 Accuracy: 0.90625\n",
      "Loss: 0.5202920436859131 Accuracy: 0.859375\n",
      "Loss: 0.5544601082801819 Accuracy: 0.8359375\n",
      "Final test accuracy: 0.8889999985694885\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "\n",
    "# uhm, maybe don't use that many layers actually. 2 is fine!\n",
    "n_units = 100\n",
    "n_layers = 2\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, 0.),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]\n",
    "\n",
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, 0.),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, 0.),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]\n",
    "\n",
    "\n",
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.leaky_relu(tf.matmul(x, w) + b)\n",
    "    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
    "\n",
    "\n",
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Analysing Gradient values for Weights \n",
    "for gradients in range(0, 6, 2):\n",
    "    print(tf.reduce_mean(grads[gradients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-2.7846037e-05, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0006243043, shape=(), dtype=float32)\n",
      "tf.Tensor(-7.450581e-11, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# after leaky relu\n",
    "for gradients in range(0, 6, 2):\n",
    "    print(tf.reduce_mean(grads[gradients]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 3: Dying ReLU problem as the mean of gradients is  0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2976608276367188 Accuracy: 0.1328125\n",
      "Loss: 0.7102692127227783 Accuracy: 0.78125\n",
      "Loss: 0.41992825269699097 Accuracy: 0.8515625\n",
      "Loss: 0.30277734994888306 Accuracy: 0.8984375\n",
      "Loss: 0.28708428144454956 Accuracy: 0.9140625\n",
      "Loss: 0.2686609625816345 Accuracy: 0.921875\n",
      "Loss: 0.3374359607696533 Accuracy: 0.8984375\n",
      "Loss: 0.22945359349250793 Accuracy: 0.921875\n",
      "Loss: 0.3092650771141052 Accuracy: 0.890625\n",
      "Loss: 0.38359490036964417 Accuracy: 0.890625\n",
      "Loss: 0.3187181055545807 Accuracy: 0.9296875\n",
      "Loss: 0.1970217227935791 Accuracy: 0.9296875\n",
      "Loss: 0.14141243696212769 Accuracy: 0.9375\n",
      "Loss: 0.1671806275844574 Accuracy: 0.9375\n",
      "Loss: 0.21383097767829895 Accuracy: 0.953125\n",
      "Loss: 0.19383956491947174 Accuracy: 0.9453125\n",
      "Loss: 0.16973355412483215 Accuracy: 0.953125\n",
      "Loss: 0.2713935673236847 Accuracy: 0.9296875\n",
      "Loss: 0.13440853357315063 Accuracy: 0.953125\n",
      "Loss: 0.13311249017715454 Accuracy: 0.9453125\n",
      "Loss: 0.12100504338741302 Accuracy: 0.96875\n",
      "Final test accuracy: 0.9558999538421631\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "\n",
    "# 2 layers again\n",
    "n_units = 100\n",
    "n_layers = 2\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]\n",
    "\n",
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]\n",
    "\n",
    "\n",
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n",
    "\n",
    "    return logits\n",
    "\n",
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    # I hear that adding noise to the inputs improves generalization!\n",
    "    #img_batch += tf.random.normal(tf.shape(img_batch), stddev=1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
    "\n",
    "\n",
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 4: Removing noise worked better for generalisation error in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2788195610046387 Accuracy: 0.2265625\n",
      "Loss: 0.7461198568344116 Accuracy: 0.765625\n",
      "Loss: 0.45109856128692627 Accuracy: 0.859375\n",
      "Loss: 0.4517952799797058 Accuracy: 0.8359375\n",
      "Loss: 0.16791829466819763 Accuracy: 0.9609375\n",
      "Loss: 0.2859359681606293 Accuracy: 0.921875\n",
      "Loss: 0.28815436363220215 Accuracy: 0.890625\n",
      "Loss: 0.28165754675865173 Accuracy: 0.9140625\n",
      "Loss: 0.22864007949829102 Accuracy: 0.9453125\n",
      "Loss: 0.15202617645263672 Accuracy: 0.9453125\n",
      "Loss: 0.2008635401725769 Accuracy: 0.9453125\n",
      "Loss: 0.1860160082578659 Accuracy: 0.9453125\n",
      "Loss: 0.1460484266281128 Accuracy: 0.9453125\n",
      "Loss: 0.15097488462924957 Accuracy: 0.953125\n",
      "Loss: 0.46626636385917664 Accuracy: 0.8671875\n",
      "Loss: 0.13847529888153076 Accuracy: 0.9609375\n",
      "Loss: 0.18282972276210785 Accuracy: 0.96875\n",
      "Loss: 0.1205345094203949 Accuracy: 0.9765625\n",
      "Loss: 0.07197416573762894 Accuracy: 0.984375\n",
      "Loss: 0.15321514010429382 Accuracy: 0.96875\n",
      "Loss: 0.14835239946842194 Accuracy: 0.953125\n",
      "Final test accuracy: 0.9560999870300293\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "\n",
    "# 2 layers again\n",
    "n_units = 100\n",
    "n_layers = 2\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]\n",
    "\n",
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]\n",
    "\n",
    "\n",
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "    # finally, the softmax classification output layer :)))\n",
    "    logits = (tf.matmul(x, layers[-1][0]) + layers[-1][1])\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
    "\n",
    "\n",
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail 5: sparse_softmax_cross_entropy internally applies softmax onto logits and requires raw logit output i.e before softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
